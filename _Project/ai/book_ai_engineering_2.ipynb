{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7dc290f6",
   "metadata": {},
   "source": [
    "# ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ \n",
    "\n",
    "- ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ì˜ í•µì‹¬ ì•„ì´ë””ì–´ëŠ” ê° ì²­í¬ì— ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸ë¥¼ ì¶”ê°€í•´ í•„ìš”í•œ ì²­í¬ë¥¼ ë” ì‰½ê²Œ ê²€ìƒ‰í•  ìˆ˜ ìˆê²Œ í•˜ëŠ” ê²ƒì´ë‹¤. \n",
    "  - ê°„ë‹¨í•œ ë°©ë²•ì€ íƒœê·¸ë‚˜ í‚¤ì›Œë“œ ê°™ì€ ë©”íƒ€ë°ì´í„°ë¡œ ì²­í¬ë¥¼ ë³´ê°•í•˜ëŠ” í•˜ëŠ” ê²ƒì´ë‹¤. \n",
    "  - ì „ì ìƒê±°ë˜ì˜ ê²½ìš°, ìƒí’ˆì— ì„¤ëª…ê³¼ ë¦¬ë·°ë¥¼ í•¨ê»˜ ì €ì¥í•  ìˆ˜ ìˆë‹¤. \n",
    "  - ê° ì²­í¬ì— ì§ˆì˜ë“¤ì„ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤. ê³ ê° ì§€ì›ì˜ ê²½ìš°, ê° ë¬¸ì„œì— ê´€ë ¨ ì§ˆì˜ë“¤ì„ ì¶”ê°€í•  ìˆ˜ ìˆë‹¤. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "239eabf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (Contextual Retrieval) ì‹œìŠ¤í…œ\n",
    "\n",
    "í•µì‹¬ ì•„ì´ë””ì–´: ê° ì²­í¬ì— ê´€ë ¨ ì»¨í…ìŠ¤íŠ¸(ë©”íƒ€ë°ì´í„°, í‚¤ì›Œë“œ, ì§ˆì˜)ë¥¼ ì¶”ê°€í•˜ì—¬\n",
    "ê²€ìƒ‰ í’ˆì§ˆì„ í–¥ìƒì‹œí‚´\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass, field\n",
    "from typing import Optional\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "import re\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class EnrichedChunk:\n",
    "    \"\"\"ì»¨í…ìŠ¤íŠ¸ê°€ ë³´ê°•ëœ ì²­í¬\"\"\"\n",
    "    id: str\n",
    "    content: str  # ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "    \n",
    "    # ë©”íƒ€ë°ì´í„°\n",
    "    tags: list[str] = field(default_factory=list)\n",
    "    keywords: list[str] = field(default_factory=list)\n",
    "    category: Optional[str] = None\n",
    "    \n",
    "    # ê´€ë ¨ ì§ˆì˜ë“¤ (ì´ ì²­í¬ê°€ ë‹µë³€í•  ìˆ˜ ìˆëŠ” ì§ˆë¬¸ë“¤)\n",
    "    related_queries: list[str] = field(default_factory=list)\n",
    "    \n",
    "    # ì¶”ê°€ ì»¨í…ìŠ¤íŠ¸\n",
    "    summary: Optional[str] = None\n",
    "    parent_context: Optional[str] = None  # ìƒìœ„ ë¬¸ì„œì˜ ì»¨í…ìŠ¤íŠ¸\n",
    "    \n",
    "    def get_searchable_text(self) -> str:\n",
    "        \"\"\"ê²€ìƒ‰ì— ì‚¬ìš©ë  í™•ì¥ëœ í…ìŠ¤íŠ¸ ë°˜í™˜\"\"\"\n",
    "        parts = [self.content]\n",
    "        \n",
    "        if self.tags:\n",
    "            parts.append(f\"íƒœê·¸: {', '.join(self.tags)}\")\n",
    "        if self.keywords:\n",
    "            parts.append(f\"í‚¤ì›Œë“œ: {', '.join(self.keywords)}\")\n",
    "        if self.related_queries:\n",
    "            parts.append(f\"ê´€ë ¨ ì§ˆì˜: {' | '.join(self.related_queries)}\")\n",
    "        if self.summary:\n",
    "            parts.append(f\"ìš”ì•½: {self.summary}\")\n",
    "        if self.parent_context:\n",
    "            parts.append(f\"ë¬¸ë§¥: {self.parent_context}\")\n",
    "            \n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ì˜ˆì‹œ 1: ì „ììƒê±°ë˜ - ìƒí’ˆ ì •ë³´ + ë¦¬ë·°\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass\n",
    "class ProductChunk(EnrichedChunk):\n",
    "    \"\"\"ì „ììƒê±°ë˜ìš© ìƒí’ˆ ì²­í¬\"\"\"\n",
    "    product_name: Optional[str] = None\n",
    "    price: Optional[float] = None\n",
    "    reviews: list[str] = field(default_factory=list)\n",
    "    rating: Optional[float] = None\n",
    "    \n",
    "    def get_searchable_text(self) -> str:\n",
    "        base = super().get_searchable_text()\n",
    "        parts = [base]\n",
    "        \n",
    "        if self.reviews:\n",
    "            parts.append(f\"ë¦¬ë·°: {' '.join(self.reviews)}\")\n",
    "        if self.rating:\n",
    "            parts.append(f\"í‰ì : {self.rating}\")\n",
    "            \n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def create_product_chunk(\n",
    "    product_id: str,\n",
    "    name: str,\n",
    "    description: str,\n",
    "    reviews: list[str],\n",
    "    price: float,\n",
    "    rating: float\n",
    ") -> ProductChunk:\n",
    "    \"\"\"ìƒí’ˆ ì •ë³´ë¡œë¶€í„° ë³´ê°•ëœ ì²­í¬ ìƒì„±\"\"\"\n",
    "    \n",
    "    # í‚¤ì›Œë“œ ìë™ ì¶”ì¶œ (ê°„ë‹¨í•œ ë°©ì‹)\n",
    "    all_text = f\"{name} {description} {' '.join(reviews)}\"\n",
    "    keywords = extract_keywords(all_text)\n",
    "    \n",
    "    # ê´€ë ¨ ì§ˆì˜ ìë™ ìƒì„±\n",
    "    related_queries = [\n",
    "        f\"{name} ê°€ê²©\",\n",
    "        f\"{name} ë¦¬ë·°\",\n",
    "        f\"{name} ì¶”ì²œ\",\n",
    "        f\"{name} ì¥ë‹¨ì \",\n",
    "    ]\n",
    "    \n",
    "    # ë¦¬ë·°ì—ì„œ íƒœê·¸ ì¶”ì¶œ\n",
    "    tags = extract_sentiment_tags(reviews)\n",
    "    \n",
    "    return ProductChunk(\n",
    "        id=product_id,\n",
    "        content=description,\n",
    "        product_name=name,\n",
    "        price=price,\n",
    "        reviews=reviews,\n",
    "        rating=rating,\n",
    "        keywords=keywords,\n",
    "        related_queries=related_queries,\n",
    "        tags=tags,\n",
    "        summary=f\"{name} - {rating}ì , {len(reviews)}ê°œ ë¦¬ë·°\"\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ì˜ˆì‹œ 2: ê³ ê° ì§€ì› - FAQ ë¬¸ì„œ + ê´€ë ¨ ì§ˆì˜\n",
    "# =============================================================================\n",
    "\n",
    "@dataclass \n",
    "class SupportChunk(EnrichedChunk):\n",
    "    \"\"\"ê³ ê° ì§€ì›ìš© ë¬¸ì„œ ì²­í¬\"\"\"\n",
    "    document_title: Optional[str] = None\n",
    "    solution_steps: list[str] = field(default_factory=list)\n",
    "    \n",
    "    def get_searchable_text(self) -> str:\n",
    "        base = super().get_searchable_text()\n",
    "        parts = [base]\n",
    "        \n",
    "        if self.solution_steps:\n",
    "            parts.append(f\"í•´ê²° ë°©ë²•: {' -> '.join(self.solution_steps)}\")\n",
    "            \n",
    "        return \"\\n\".join(parts)\n",
    "\n",
    "\n",
    "def create_support_chunk(\n",
    "    doc_id: str,\n",
    "    title: str,\n",
    "    content: str,\n",
    "    user_queries: list[str],  # ì‹¤ì œ ê³ ê°ë“¤ì´ í–ˆë˜ ì§ˆë¬¸ë“¤\n",
    "    solution_steps: list[str] = None\n",
    ") -> SupportChunk:\n",
    "    \"\"\"ê³ ê° ì§€ì› ë¬¸ì„œë¡œë¶€í„° ë³´ê°•ëœ ì²­í¬ ìƒì„±\"\"\"\n",
    "    \n",
    "    keywords = extract_keywords(f\"{title} {content}\")\n",
    "    \n",
    "    # ì§ˆì˜ ë³€í˜• ìƒì„± (ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹)\n",
    "    expanded_queries = expand_queries(user_queries)\n",
    "    \n",
    "    return SupportChunk(\n",
    "        id=doc_id,\n",
    "        content=content,\n",
    "        document_title=title,\n",
    "        keywords=keywords,\n",
    "        related_queries=expanded_queries,\n",
    "        solution_steps=solution_steps or [],\n",
    "        tags=[\"ê³ ê°ì§€ì›\", \"FAQ\"],\n",
    "        summary=title\n",
    "    )\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ìœ í‹¸ë¦¬í‹° í•¨ìˆ˜ë“¤\n",
    "# =============================================================================\n",
    "\n",
    "def extract_keywords(text: str, top_n: int = 10) -> list[str]:\n",
    "    \"\"\"í…ìŠ¤íŠ¸ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ê°„ë‹¨í•œ ë¹ˆë„ ê¸°ë°˜)\"\"\"\n",
    "    # ë¶ˆìš©ì–´ (ì‹¤ì œë¡œëŠ” ë” ë§ì´ í•„ìš”)\n",
    "    stopwords = {'ì´', 'ê·¸', 'ì €', 'ê²ƒ', 'ìˆ˜', 'ë“±', 'ë°', 'ì˜', 'ë¥¼', 'ì„', 'ì—', 'ê°€', \n",
    "                 'ì€', 'ëŠ”', 'ìœ¼ë¡œ', 'ë¡œ', 'ì™€', 'ê³¼', 'ì—ì„œ', 'í•˜ëŠ”', 'í•˜ë‹¤', 'ìˆë‹¤',\n",
    "                 'the', 'a', 'an', 'is', 'are', 'was', 'were', 'be', 'been', 'and', 'or'}\n",
    "    \n",
    "    # ë‹¨ì–´ ì¶”ì¶œ (í•œê¸€ + ì˜ì–´)\n",
    "    words = re.findall(r'[ê°€-í£]+|[a-zA-Z]+', text.lower())\n",
    "    \n",
    "    # ë¶ˆìš©ì–´ ì œê±° ë° ì§§ì€ ë‹¨ì–´ ì œê±°\n",
    "    words = [w for w in words if w not in stopwords and len(w) > 1]\n",
    "    \n",
    "    # ë¹ˆë„ ê³„ì‚°\n",
    "    counter = Counter(words)\n",
    "    \n",
    "    return [word for word, _ in counter.most_common(top_n)]\n",
    "\n",
    "\n",
    "def extract_sentiment_tags(reviews: list[str]) -> list[str]:\n",
    "    \"\"\"ë¦¬ë·°ì—ì„œ ê°ì„± íƒœê·¸ ì¶”ì¶œ\"\"\"\n",
    "    tags = []\n",
    "    all_reviews = \" \".join(reviews).lower()\n",
    "    \n",
    "    positive_words = ['ì¢‹', 'ë§Œì¡±', 'ì¶”ì²œ', 'ìµœê³ ', 'í›Œë¥­', 'í¸ë¦¬', 'ë¹ ë¥¸', 'good', 'great', 'excellent']\n",
    "    negative_words = ['ë‚˜ìœ', 'ë¶ˆë§Œ', 'ë³„ë¡œ', 'ì‹¤ë§', 'ëŠë¦°', 'bad', 'poor', 'terrible']\n",
    "    \n",
    "    for word in positive_words:\n",
    "        if word in all_reviews:\n",
    "            tags.append(\"ê¸ì •ì \")\n",
    "            break\n",
    "            \n",
    "    for word in negative_words:\n",
    "        if word in all_reviews:\n",
    "            tags.append(\"ê°œì„ í•„ìš”\")\n",
    "            break\n",
    "            \n",
    "    return tags if tags else [\"ì¤‘ë¦½\"]\n",
    "\n",
    "\n",
    "def expand_queries(queries: list[str]) -> list[str]:\n",
    "    \"\"\"ì§ˆì˜ë¥¼ ë‹¤ì–‘í•œ í˜•íƒœë¡œ í™•ì¥\"\"\"\n",
    "    expanded = list(queries)  # ì›ë³¸ ìœ ì§€\n",
    "    \n",
    "    for query in queries:\n",
    "        # ì§ˆë¬¸ í˜•íƒœ ë³€í˜•\n",
    "        if \"ì–´ë–»ê²Œ\" not in query:\n",
    "            expanded.append(f\"{query} ì–´ë–»ê²Œ\")\n",
    "        if \"ë°©ë²•\" not in query:\n",
    "            expanded.append(f\"{query} ë°©ë²•\")\n",
    "        if \"?\" not in query:\n",
    "            expanded.append(f\"{query}?\")\n",
    "            \n",
    "    return list(set(expanded))  # ì¤‘ë³µ ì œê±°\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ê²€ìƒ‰ ì‹œìŠ¤í…œ\n",
    "# =============================================================================\n",
    "\n",
    "class ContextualRetriever:\n",
    "    \"\"\"ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ ê²€ìƒ‰ê¸°\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.chunks: list[EnrichedChunk] = []\n",
    "        \n",
    "    def add_chunk(self, chunk: EnrichedChunk):\n",
    "        \"\"\"ì²­í¬ ì¶”ê°€\"\"\"\n",
    "        self.chunks.append(chunk)\n",
    "        \n",
    "    def add_chunks(self, chunks: list[EnrichedChunk]):\n",
    "        \"\"\"ì—¬ëŸ¬ ì²­í¬ ì¶”ê°€\"\"\"\n",
    "        self.chunks.extend(chunks)\n",
    "        \n",
    "    def search(self, query: str, top_k: int = 5) -> list[tuple[EnrichedChunk, float]]:\n",
    "        \"\"\"\n",
    "        ê²€ìƒ‰ ìˆ˜í–‰ (ê°„ë‹¨í•œ í‚¤ì›Œë“œ ë§¤ì¹­ ê¸°ë°˜)\n",
    "        ì‹¤ì œ êµ¬í˜„ì—ì„œëŠ” ì„ë² ë”© ê¸°ë°˜ ê²€ìƒ‰ ì‚¬ìš© ê¶Œì¥\n",
    "        \"\"\"\n",
    "        query_keywords = set(extract_keywords(query, top_n=20))\n",
    "        results = []\n",
    "        \n",
    "        for chunk in self.chunks:\n",
    "            # í™•ì¥ëœ ê²€ìƒ‰ í…ìŠ¤íŠ¸ ì‚¬ìš©\n",
    "            searchable_text = chunk.get_searchable_text().lower()\n",
    "            chunk_keywords = set(extract_keywords(searchable_text, top_n=50))\n",
    "            \n",
    "            # ê´€ë ¨ ì§ˆì˜ì™€ì˜ ë§¤ì¹­ ì ìˆ˜ (ë³´ë„ˆìŠ¤)\n",
    "            query_match_bonus = 0\n",
    "            for related_query in chunk.related_queries:\n",
    "                if any(kw in related_query.lower() for kw in query_keywords):\n",
    "                    query_match_bonus += 0.2\n",
    "                    \n",
    "            # í‚¤ì›Œë“œ ê²¹ì¹¨ ì ìˆ˜\n",
    "            overlap = len(query_keywords & chunk_keywords)\n",
    "            score = overlap / max(len(query_keywords), 1) + query_match_bonus\n",
    "            \n",
    "            results.append((chunk, score))\n",
    "            \n",
    "        # ì ìˆ˜ìˆœ ì •ë ¬\n",
    "        results.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        return results[:top_k]\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# ì‚¬ìš© ì˜ˆì‹œ\n",
    "# =============================================================================\n",
    "\n",
    "def demo_ecommerce():\n",
    "    \"\"\"ì „ììƒê±°ë˜ ì˜ˆì‹œ\"\"\"\n",
    "    print(\"=\" * 60)\n",
    "    print(\"ğŸ“¦ ì „ììƒê±°ë˜ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì˜ˆì‹œ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ìƒí’ˆ ì²­í¬ ìƒì„±\n",
    "    products = [\n",
    "        create_product_chunk(\n",
    "            product_id=\"P001\",\n",
    "            name=\"ë¬´ì„  ë¸”ë£¨íˆ¬ìŠ¤ ì´ì–´í°\",\n",
    "            description=\"ê³ ìŒì§ˆ ë…¸ì´ì¦ˆ ìº”ìŠ¬ë§ ê¸°ëŠ¥ì´ íƒ‘ì¬ëœ ì™„ì „ ë¬´ì„  ì´ì–´í°ì…ë‹ˆë‹¤. ë°°í„°ë¦¬ ìˆ˜ëª… 8ì‹œê°„, ì¶©ì „ ì¼€ì´ìŠ¤ í¬í•¨.\",\n",
    "            reviews=[\n",
    "                \"ìŒì§ˆì´ ì •ë§ ì¢‹ì•„ìš”! ë…¸ì´ì¦ˆ ìº”ìŠ¬ë§ ìµœê³ ì…ë‹ˆë‹¤.\",\n",
    "                \"ë°°í„°ë¦¬ê°€ ì˜¤ë˜ê°€ì„œ ë§Œì¡±í•´ìš”.\",\n",
    "                \"ê°€ì„±ë¹„ ì¶”ì²œí•©ë‹ˆë‹¤.\"\n",
    "            ],\n",
    "            price=89000,\n",
    "            rating=4.5\n",
    "        ),\n",
    "        create_product_chunk(\n",
    "            product_id=\"P002\", \n",
    "            name=\"ê¸°ê³„ì‹ í‚¤ë³´ë“œ\",\n",
    "            description=\"ì²­ì¶• ìŠ¤ìœ„ì¹˜ ê¸°ê³„ì‹ í‚¤ë³´ë“œ. RGB ë°±ë¼ì´íŠ¸, í…í‚¤ë¦¬ìŠ¤ 87í‚¤ ë ˆì´ì•„ì›ƒ.\",\n",
    "            reviews=[\n",
    "                \"íƒ€ê±´ê°ì´ ì¢‹ìŠµë‹ˆë‹¤. ì²­ì¶• íŠ¹ìœ ì˜ í´ë¦­ìŒì´ ì‹œì›í•´ìš”.\",\n",
    "                \"RGB ì¡°ëª…ì´ ì˜ˆì˜ë„¤ìš”.\",\n",
    "                \"ì‚¬ë¬´ì‹¤ì—ì„œ ì“°ê¸°ì—” ì†ŒìŒì´ ì¢€ ìˆì–´ìš”.\"\n",
    "            ],\n",
    "            price=79000,\n",
    "            rating=4.2\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    # ê²€ìƒ‰ê¸° ìƒì„± ë° ì²­í¬ ì¶”ê°€\n",
    "    retriever = ContextualRetriever()\n",
    "    retriever.add_chunks(products)\n",
    "    \n",
    "    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸\n",
    "    queries = [\n",
    "        \"ë…¸ì´ì¦ˆ ìº”ìŠ¬ë§ ì´ì–´í° ì¶”ì²œ\",\n",
    "        \"íƒ€ê±´ê° ì¢‹ì€ í‚¤ë³´ë“œ\",\n",
    "        \"ë°°í„°ë¦¬ ì˜¤ë˜ê°€ëŠ” ì´ì–´í°\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nğŸ” ê²€ìƒ‰: '{query}'\")\n",
    "        results = retriever.search(query, top_k=2)\n",
    "        \n",
    "        for chunk, score in results:\n",
    "            print(f\"  â†’ {chunk.product_name} (ì ìˆ˜: {score:.2f})\")\n",
    "            print(f\"     í‚¤ì›Œë“œ: {', '.join(chunk.keywords[:5])}\")\n",
    "            print(f\"     í‰ì : {chunk.rating}\")\n",
    "\n",
    "\n",
    "def demo_support():\n",
    "    \"\"\"ê³ ê° ì§€ì› ì˜ˆì‹œ\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ§ ê³ ê° ì§€ì› ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì˜ˆì‹œ\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ì§€ì› ë¬¸ì„œ ì²­í¬ ìƒì„±\n",
    "    docs = [\n",
    "        create_support_chunk(\n",
    "            doc_id=\"D001\",\n",
    "            title=\"ë¹„ë°€ë²ˆí˜¸ ì¬ì„¤ì • ë°©ë²•\",\n",
    "            content=\"ë¹„ë°€ë²ˆí˜¸ë¥¼ ìŠì–´ë²„ë¦° ê²½ìš°, ë¡œê·¸ì¸ í˜ì´ì§€ì—ì„œ 'ë¹„ë°€ë²ˆí˜¸ ì°¾ê¸°'ë¥¼ í´ë¦­í•˜ì—¬ ì¬ì„¤ì •í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "            user_queries=[\n",
    "                \"ë¹„ë°€ë²ˆí˜¸ë¥¼ ìŠì–´ë²„ë ¸ì–´ìš”\",\n",
    "                \"ë¡œê·¸ì¸ì´ ì•ˆë¼ìš”\",\n",
    "                \"ë¹„ë°€ë²ˆí˜¸ ë³€ê²½í•˜ê³  ì‹¶ì–´ìš”\"\n",
    "            ],\n",
    "            solution_steps=[\"ë¡œê·¸ì¸ í˜ì´ì§€ ì ‘ì†\", \"'ë¹„ë°€ë²ˆí˜¸ ì°¾ê¸°' í´ë¦­\", \"ì´ë©”ì¼ ì…ë ¥\", \"ì¸ì¦ ë©”ì¼ í™•ì¸\", \"ìƒˆ ë¹„ë°€ë²ˆí˜¸ ì„¤ì •\"]\n",
    "        ),\n",
    "        create_support_chunk(\n",
    "            doc_id=\"D002\",\n",
    "            title=\"í™˜ë¶ˆ ì •ì±… ì•ˆë‚´\",\n",
    "            content=\"ìƒí’ˆ ìˆ˜ë ¹ í›„ 7ì¼ ì´ë‚´ì— í™˜ë¶ˆ ì‹ ì²­ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. ë‹¨, ì‚¬ìš© í”ì ì´ ìˆëŠ” ê²½ìš° í™˜ë¶ˆì´ ì œí•œë  ìˆ˜ ìˆìŠµë‹ˆë‹¤.\",\n",
    "            user_queries=[\n",
    "                \"í™˜ë¶ˆí•˜ê³  ì‹¶ì–´ìš”\",\n",
    "                \"ë°˜í’ˆ ì–´ë–»ê²Œ í•´ìš”\",\n",
    "                \"êµí™˜ ê°€ëŠ¥í•œê°€ìš”\"\n",
    "            ],\n",
    "            solution_steps=[\"ë§ˆì´í˜ì´ì§€ ì ‘ì†\", \"ì£¼ë¬¸ ë‚´ì—­ í™•ì¸\", \"í™˜ë¶ˆ ì‹ ì²­ í´ë¦­\", \"ì‚¬ìœ  ì„ íƒ\", \"ì‹ ì²­ ì™„ë£Œ\"]\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    retriever = ContextualRetriever()\n",
    "    retriever.add_chunks(docs)\n",
    "    \n",
    "    # ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ (ë‹¤ì–‘í•œ í‘œí˜„ ë°©ì‹)\n",
    "    queries = [\n",
    "        \"ë¹„ë°€ë²ˆí˜¸ê°€ ê¸°ì–µ ì•ˆë‚˜ìš”\",\n",
    "        \"ë¬¼ê±´ ë°˜í’ˆí•˜ë ¤ë©´\",\n",
    "        \"ë¡œê·¸ì¸ ë¬¸ì œ\"\n",
    "    ]\n",
    "    \n",
    "    for query in queries:\n",
    "        print(f\"\\nğŸ” ê²€ìƒ‰: '{query}'\")\n",
    "        results = retriever.search(query, top_k=1)\n",
    "        \n",
    "        for chunk, score in results:\n",
    "            if isinstance(chunk, SupportChunk):\n",
    "                print(f\"  â†’ {chunk.document_title} (ì ìˆ˜: {score:.2f})\")\n",
    "                print(f\"     ê´€ë ¨ ì§ˆì˜: {chunk.related_queries[:3]}\")\n",
    "                if chunk.solution_steps:\n",
    "                    print(f\"     í•´ê²° ë‹¨ê³„: {' â†’ '.join(chunk.solution_steps[:3])}...\")\n",
    "\n",
    "\n",
    "def demo_chunk_enrichment():\n",
    "    \"\"\"ì²­í¬ ë³´ê°• ê³¼ì • ì‹œê°í™”\"\"\"\n",
    "    print(\"\\n\" + \"=\" * 60)\n",
    "    print(\"ğŸ“ ì²­í¬ ë³´ê°• ê³¼ì •\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # ì›ë³¸ í…ìŠ¤íŠ¸\n",
    "    original = \"ê³ ìŒì§ˆ ë…¸ì´ì¦ˆ ìº”ìŠ¬ë§ ê¸°ëŠ¥ì´ íƒ‘ì¬ëœ ì™„ì „ ë¬´ì„  ì´ì–´í°ì…ë‹ˆë‹¤.\"\n",
    "    \n",
    "    print(f\"\\n[ì›ë³¸ ì²­í¬]\")\n",
    "    print(f\"  {original}\")\n",
    "    \n",
    "    # ë³´ê°•ëœ ì²­í¬\n",
    "    enriched = create_product_chunk(\n",
    "        product_id=\"P001\",\n",
    "        name=\"ë¬´ì„  ë¸”ë£¨íˆ¬ìŠ¤ ì´ì–´í°\",\n",
    "        description=original,\n",
    "        reviews=[\"ìŒì§ˆ ì¢‹ì•„ìš”\", \"ë…¸ì´ì¦ˆ ìº”ìŠ¬ë§ ìµœê³ \"],\n",
    "        price=89000,\n",
    "        rating=4.5\n",
    "    )\n",
    "    \n",
    "    print(f\"\\n[ë³´ê°•ëœ ì²­í¬ - ê²€ìƒ‰ìš© í…ìŠ¤íŠ¸]\")\n",
    "    print(\"-\" * 40)\n",
    "    print(enriched.get_searchable_text())\n",
    "    print(\"-\" * 40)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demo_chunk_enrichment()\n",
    "    demo_ecommerce()\n",
    "    demo_support()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
