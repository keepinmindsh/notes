# LLM Ops

- ë‚´ì¥ LLM ê¸°ëŠ¥ 
  - AIaas & ì‚¬ì „ í•™ìŠµ / ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸
- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ 
  - AIaas & ì˜¤í”ˆ ì†ŒìŠ¤ / instruction-tuned
- ë‚´ë¶€ì§€ì‹ ì¶”ê°€ 
  - LLMì´ ë‹µë³€ì— ì‚¬ìš©í•™ ìœ„í•œ ì»¨í…ì¸ ì˜ ì‚¬ì „ ì¤€ë¹„ 
- ê³ ê¸‰ê¸°ë²• 
  - ì¼ë ¨ì˜ ëª¨ë¸ì— ì˜í•œ íŒŒì¸íŠœë‹ / ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ 


# RAG ê²€ì¦ ê°œì„  

- RAG ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ë°©ë²•ë¡ ì€ ì •ë§ ë„ˆë¬´ë‚˜ë„ ë§ë‹¤. 
- ë­ì²´ì¸ì´ë‚˜ ë¼ë§ˆì¸ë±ìŠ¤ì— êµ¬í˜„ë˜ì–´ ìˆëŠ” ê²ƒë“¤ì€ ë¹™ì‚°ì˜ ì¼ê° 


# RAG í‰ê°€ ë°©ë²• 

- ì§ˆë¬¸ 
- ë‹¨ë½
  - Retrieval í‰ê°€ 
  - Retrieve ëœ ë‹¨ë½ A, B, Cê°€ retrieval gt Aì™€ ì¼ì¹˜í•˜ëŠ”ê°€?
- ìƒì„± 
  - ìƒì„±í•œ ë‹µë³€ í‰ê°€ 
  - ìƒì„±í•œ ë‹µë³€ Aê°€ generation gt Bì™€ ìœ ì‚¬í•œê°€ 


## Context Recall 

## Context Precision

## Ground Truthë¥¼ ì–´ë–»ê²Œ ë§Œë“œëŠ”ê°€? 

- Simple Gen
- RAGAS Gen 


# ì¢‹ì€ RAG í‰ê°€ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” ë°©ë²• 


- ë°ì´í„° ìƒì„±ì„ ì „ë¶€ ìë™í™”í•˜ëŠ” ê²ƒì€ ê±°ì˜ ë¶ˆê°€ëŠ¥ 
- ê°€ì¥ ì¢‹ì€ ë°©ë²•ì€ "ì‹¤ì œ ìœ ì € ë°ì´í„° í™œìš©"
- ë‘ë²ˆì§¸ë¡œëŠ” "ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ ë§Œë“œëŠ” ê²ƒ"
- ì„¸ë²ˆì§¸ ì˜µì…˜ì´ "Human In the Loop" ì „ëµ 
- í‰ê°€ ë°ì´í„° ì…‹ì„ ë§Œë“œëŠ” ì¼ì€ ê³ í†µ ìŠ¤ëŸ½ì§€ë§Œ, ë§ì€ ì‹œê°„ê³¼ ë…¸ë ¥ì„ íˆ¬ì…í•´ì•¼í•¨ 

# Human in the loop QA Gen

- LLMìœ¼ë¡œ QA ìƒì„± 
- ì‚¬ëŒì˜ ê²€ì¦ 
- í”„ë¡¬í”„íŠ¸ ì¬ì‘ì„± 

# LLM as Judge 

- í‰ê°€ì˜ ëª©ì 
  - Prompt for Evaluation 
    - ì–¸ì–´ëª¨ë¸ì— ì˜í•´ì„œ í‰ê°€í•˜ì 

- The default prompt for pairwise comparison

``` 
System
Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question below. 
Please read the userâ€™s question and the provided responses. 
Then follow the instructions and answer the userâ€™s question better. 
Provide explanations for your reasons such as the helpfulness, 
relevance, accuracy, depth, creativity, and level of detail of their responses. 
Begin your evaluation by comparing the two responses and provide a short explanation. 
Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. 
Do not allow the length of the responses to influence your evaluation. 
Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, 
output your final verdict by strictly using the format: â€œ[answerA]â€ if assistant A is better, â€œ[answerB]â€ if assistant B is better, and â€œ[C]â€ for a tie.

User Question
[question]

The Start of Assistant Aâ€™s Answer
A
[The End of Assistant Aâ€™s Answer]

The Start of Assistant Bâ€™s Answer
B
[The End of Assistant Bâ€™s Answer]

Figure 5: The default prompt for evaluation
```

``` 
[System]
You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.

[User Input]
Select the Output (a) or Output (b) that is better for the given instruction. 
You should answer only using "Output (a)" or "Output (b). DO NOT output any other words.

Here are some rules of the evaluation:
(1) You should interpret whether the output honestly/precisely/closely follows the instruction, consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should not contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. Do not presume that the order in which the outputs were presented should not affect your judgment, as Output (a) and Output (b) are **equally likely** to be the better one.

DO NOT provide any explanation for your choice.
DO NOT say both are nearly good.
You should answer using only "Output (a)" or "Output (b). DO NOT output any other words.

# Instruction:
{context}

# Output (a):
{output1}

# Output (b):
{output2}

# Which is better, Output (a) or Output (b)? Your response should be either "Output (a)" or "Output (b)".
```

> https://lmarena.ai/


# í‰ê°€ì— ëŒ€í•œ ê°€ì´ë“œë¼ì¸ì´ ë„ˆë¬´ ëª¨í˜¸í•´ì„œëŠ” ì•ˆëœë‹¤.

``` 
Consider the input prompt in the <input> tags and the output answer in the <output> tags, the context in the <prompt_criteria> tags, and the answer evaluation criteria in the <answer_criteria> tags.

<prompt_criteria>
The prompt should be clear, direct, and detailed.
The question, task, or goal should be well explained and be grammatically correct.
The prompt is better if containing examples.
The prompt is better if it specifies a role or sets a context.
The prompt is better if it provides details about the format and tone of the expected answer.
</prompt_criteria>
```

``` 
answer-score : ë‹µë³€ í‰ê°€ ì ìˆ˜ ( 0 ~ 100 ) - í™˜ê°ì´ ìˆìœ¼ë©´ ì ìˆ˜ê°€ í¬ê²Œ ê°ì†Œ 
prompt-score : í”„ë¡¬í”„íŠ¸ í‰ê°€ ì ìˆ˜ ( 0 ~ 100 ) 
justification : í‰ê°€ì— ëŒ€í•œ ì´ìœ  
input ë° output : í‰ê°€ ëŒ€ìƒ ì½˜í…ì¸ ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ 
prompt-recommendations : í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ê¶Œì¥ ì‚¬í•­ 
```

# í‰ê°€ í™˜ê²½ ì„¤ì • 

- Function Call í•¨ìˆ˜ ì •ì˜

# ëª¨ë¸ë³„ë¡œ ì¢‹ì•„í•˜ëŠ” ì ìˆ˜ ë²”ìœ„ê°€ ë”°ë¡œ ìˆì„ ìˆ˜ ìˆìŒ 

- LLMì€ ì—°ì†ì ì¸ ë²”ìœ„ í‰ê°€ì— ì•½í•˜ê¸° ë•Œë¬¸ì— LLM ë¶„ë¥˜ í‰ê°€ì— ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ

# ëª¨ë¸ì´ ì‘ì„ ìˆ˜ë¡ í¸í–¥ì´ ë” ë§ì´ ìƒê¸´ë‹¤. 

Promptì˜ ë‚´ìš©ì— ë”°ë¼ì„œ ê²°ê³¼ê°€ ë§ì´ ë‹¬ë¼ì§ˆìˆ˜ ìˆë‹¤. 

# SoftMax Function 

- https://syj9700.tistory.com/38

# vLLM Further Parameters

This document outlines the available parameters for configuring generation behavior in vLLM. These parameters provide fine-grained control over decoding, sampling, and output post-processing.

---

## ğŸ“Œ Core Output Settings

- **`n`**  
  Number of output sequences to return for the given prompt.

- **`best_of`**  
  Number of output sequences generated internally, from which the top `n` are returned. Must be `>= n`.

---

## âœï¸ Sampling & Penalization Parameters

- **`temperature`**  
  Controls randomness in sampling. Lower = more deterministic, Higher = more random.

- **`top_p`**  
  Nucleus sampling. Float âˆˆ [0, 1]; sets cumulative probability threshold.

- **`top_k`**  
  Limits sampling to the top-k tokens. `-1` means no restriction.

- **`min_p`**  
  Minimum probability for a token to be considered, relative to the most probable one.

- **`presence_penalty`**  
  Penalizes tokens already present in the output. Positive = discourage repetition.

- **`frequency_penalty`**  
  Penalizes tokens based on frequency. Positive = discourage frequent tokens.

- **`repetition_penalty`**  
  Penalizes tokens that repeat between prompt and generated output.

---

## â¹ï¸ Stopping Criteria

- **`stop`**  
  List of strings that immediately stop generation. These strings will **not** appear in output.

- **`stop_token_ids`**  
  Similar to `stop`, but for token IDs. These **will** appear unless special tokens.

- **`max_tokens`**  
  Maximum number of tokens to generate.

- **`min_tokens`**  
  Minimum tokens to generate before `stop` or `EOS` is considered.

- **`ignore_eos`**  
  Continue generation even after EOS token is generated.

---

## ğŸ“Š Log Probabilities

- **`logprobs`**  
  Number of log probabilities to return per output token.

- **`prompt_logprobs`**  
  Number of log probabilities to return per prompt token.

---

## ğŸ§¹ Output Formatting Options

- **`detokenize`**  
  Whether to detokenize the final output (default: `true`).

- **`skip_special_tokens`**  
  Whether to omit special tokens from output.

- **`spaces_between_special_tokens`**  
  Whether to insert spaces between special tokens (default: `true`).

- **`include_stop_str_in_output`**  
  Whether to include stop strings in the final output (default: `false`).

---

## ğŸ§  Advanced Options

- **`bad_words`**  
  List of disallowed token sequences.

- **`logits_processors`**  
  Custom functions to modify logits before sampling.

- **`truncate_prompt_tokens`**  
  Use only the last *k* tokens of the prompt (left truncation).

- **`guided_decoding`**  
  Use a guided decoding logits processor (optional).

- **`seed`**  
  Random seed for reproducible generation.

---

## ğŸ” Example Use

```json
{
  "prompt": "Once upon a time",
  "temperature": 0.8,
  "top_p": 0.95,
  "presence_penalty": 0.5,
  "stop": ["\n"],
  "max_tokens": 100
}
```

# Top K Sampling and its shortcomings 

í™•ë¥ ì´ ë†’ì€ ìˆœì„œì— ë”°ë¼ kê°œì˜ í† í°ì„ ë½‘ì•„ ê·¸ ì•ˆì—ì„œ ìƒ˜í”Œë§ 


# Top-p Sampling ( aka. Nucleus Sampling )

ëˆ„ì ë¶„í¬ë¥¼ ê³„ì‚°í•˜ê³  ëˆ„ì  ë¶„í¬í•¨ìˆ˜ ì‚¬ìš©ìê°€ ì§€ì •í•œ Pê°’ì„ ì´ˆê³¼í•˜ë©´ ìƒ˜í”Œë§ 

# Message Roles in Chat Context

This document describes the structure of the `messages` array used in a conversation with a generative AI model. Each element in the array represents a message and contains a `role` field that indicates the source of the message.

## ğŸ”‘ Role Types

The `role` can be one of the following:

- `"user"`  
  Indicates that the message originates from the user.

- `"assistant"`  
  Indicates that the message is a response generated by the AI assistant.

- `"system"`  
  Used to set the behavior, tone, or knowledge base of the assistant. It acts as an initial instruction to guide the modelâ€™s behavior throughout the conversation.
  

## ğŸ§¾ Example Format

```json
{
  "messages": [
    {
      "role": "system",
      "content": "You are a helpful assistant."
    },
    {
      "role": "user",
      "content": "What's the weather like today?"
    },
    {
      "role": "assistant",
      "content": "The weather is sunny and warm with a high of 25Â°C."
    }
  ]
}
```

# Assistant: Prefill

## ğŸ“Œ ëª©ì 

LLMì˜ ë‹µë³€ì„ **ë” ì •êµí•˜ê²Œ ì œì–´**í•˜ê³ , **ì˜ë„í•œ ë°©ì‹**ìœ¼ë¡œ ìœ ë„í•˜ê¸° ìœ„í•´ ì‚¬ìš©ë©ë‹ˆë‹¤.

---

## ğŸ’¡ ìš©ì–´ ì •ì˜

- **Assistant Turn**  
  AIê°€ ì‚¬ìš©ìì—ê²Œ ì‘ë‹µí•˜ëŠ” ë¶€ë¶„

- **Prefill**  
  AIê°€ **ë‹µë³€ì„ ì‹œì‘í•˜ê¸° ì „ì— ë¯¸ë¦¬ ì…ë ¥ë˜ëŠ” í…ìŠ¤íŠ¸**

---

## âœ… Prefillì˜ íš¨ê³¼

1. `"ì—¬ê¸°ê¹Œì§€ëŠ” ì´ë¯¸ ë„¤ê°€ í•œ ë§ì´ì•¼"` ë¼ëŠ” **ë§¥ë½ì„ ì•”ì‹œ**í•˜ì—¬ ëª¨ë¸ì´ ëŒ€ë‹µì˜ íë¦„ì„ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ê²Œ í•©ë‹ˆë‹¤.

2. ëª¨ë¸ì€ ì´ì „ ë°œí™”ë¥¼ ê¸°ë°˜ìœ¼ë¡œ **ìì—°ìŠ¤ëŸ½ê³  ì¼ê´€ì„± ìˆëŠ” ì‘ë‹µ**ì„ ìƒì„±í•˜ë ¤ ì‹œë„í•©ë‹ˆë‹¤.

   > ğŸ‘‰ ì´ë¥¼ í†µí•´ **AIê°€ ëŒ€í™”ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì´ì–´ê°€ë„ë¡ ì„¤ê³„**í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸ§ª ì‚¬ìš© ì˜ˆì‹œ

- ì‚¬ìš©ì:  
  `"Please respond as a formal assistant."`

- ëª¨ë¸ (Prefilled):  
  ```html
  <response>As a formal assistant, I will guide you step by step.</response>


# ğŸ“ AI-based Paraphrasing Techniques

Paraphrasing is the task of expressing the same meaning using different words or sentence structures. It is a key component in many natural language processing (NLP) applications such as summarization, chatbot response generation, question generation, and more.

---

## ğŸš€ Methods of Paraphrasing in AI

### 1. Rule-based Paraphrasing
- Utilizes grammar rules, synonym replacement, and voice transformation (active â‡„ passive).
- **Example**:  
  `He wrote a book.` â†’ `A book was written by him.`
- âœ… Simple and controllable
- âŒ Limited diversity, prone to semantic distortion

---

### 2. Machine Learning-based Paraphrasing
- Learns sentence structures and patterns from large parallel datasets.

#### Techniques:
- **Seq2Seq (RNN/LSTM)**: Encodes and decodes sequences to generate paraphrases.
- **Back-Translation**: Translates the original sentence into another language, then back to the original language.
    - Example: English â†’ French â†’ English

---

### 3. Pretrained Language Model-based Paraphrasing âœ… *Most commonly used today*
- Leverages large models like **BERT**, **GPT**, **T5**, **Pegasus**, etc.

#### Examples:
- **T5 (Text-to-Text Transfer Transformer)**  
  Input: `paraphrase: The weather is great today.`  
  Output: `It's a beautiful day.`

- **GPT-4 / ChatGPT**  
  Prompt-based generation:
  > `"Paraphrase this sentence: 'She completed the project on time.'"`  
  â†’ `"The project was finished by her within the deadline."`

---

### 4. Embedding-based Semantic Paraphrasing
- Converts sentences into embedding vectors and retrieves semantically similar sentences from a vector database.

#### Applications:
- Semantic search (e.g., FAQ matching)
- Retrieving diverse question formulations

# Tips 

- ëŒ€ë¬¸ì ì“°ì§€ ë§ ê²ƒ 
- ë¶€ì •ì–´ ì“°ì§€ ë§ ê²ƒ 
- ì„œë¹„ìŠ¤ë¥¼ ìš´ì˜í•˜ë©´ì„œ í–‰ë™ ì§€í‘œë¥¼ ë³´ëŠ” ê²ƒì´ ë§ì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŒ 
- SLMì˜ ê²½ìš°ì—ë„ ë‹¤ì–‘í•œ Referencesë¥¼ SLM Prompt ê¸°ì¤€ìœ¼ë¡œ ì°¸ì¡° í•´ì•¼í•¨. 
- ì‘ì€ ëª¨ë¸ì¼ ìˆ˜ë¡ Reasoning Stepì— ë” ë¯¼ê°í•˜ë‹¤. 

# References 

> https://www.youtube.com/watch?v=iuhXs4iHLdk&t=3275s  
> https://smith.langchain.com/hub/homanp/human-in-the-loop?organizationId=6722a1b7-48f3-4ec9-a5ac-e04155487f6a&tab=0    
> https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically   
> https://wikidocs.net/262593  
> https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/  
> https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#how-does-resuming-from-an-interrupt-work  
> https://github.com/UpstageAI/solar-prompt-cookbook