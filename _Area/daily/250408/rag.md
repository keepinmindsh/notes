# LLM Ops

- ë‚´ì¥ LLM ê¸°ëŠ¥ 
  - AIaas & ì‚¬ì „ í•™ìŠµ / ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸
- í”„ë¡¬í”„íŠ¸ ì—”ì§€ë‹ˆì–´ë§ 
  - AIaas & ì˜¤í”ˆ ì†ŒìŠ¤ / instruction-tuned
- ë‚´ë¶€ì§€ì‹ ì¶”ê°€ 
  - LLMì´ ë‹µë³€ì— ì‚¬ìš©í•™ ìœ„í•œ ì»¨í…ì¸ ì˜ ì‚¬ì „ ì¤€ë¹„ 
- ê³ ê¸‰ê¸°ë²• 
  - ì¼ë ¨ì˜ ëª¨ë¸ì— ì˜í•œ íŒŒì¸íŠœë‹ / ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜ 


# RAG ê²€ì¦ ê°œì„  

- RAG ì„±ëŠ¥ ê°œì„ ì„ ìœ„í•œ ë°©ë²•ë¡ ì€ ì •ë§ ë„ˆë¬´ë‚˜ë„ ë§ë‹¤. 
- ë­ì²´ì¸ì´ë‚˜ ë¼ë§ˆì¸ë±ìŠ¤ì— êµ¬í˜„ë˜ì–´ ìˆëŠ” ê²ƒë“¤ì€ ë¹™ì‚°ì˜ ì¼ê° 


# RAG í‰ê°€ ë°©ë²• 

- ì§ˆë¬¸ 
- ë‹¨ë½
  - Retrieval í‰ê°€ 
  - Retrieve ëœ ë‹¨ë½ A, B, Cê°€ retrieval gt Aì™€ ì¼ì¹˜í•˜ëŠ”ê°€?
- ìƒì„± 
  - ìƒì„±í•œ ë‹µë³€ í‰ê°€ 
  - ìƒì„±í•œ ë‹µë³€ Aê°€ generation gt Bì™€ ìœ ì‚¬í•œê°€ 


## Context Recall 

## Context Precision

## Ground Truthë¥¼ ì–´ë–»ê²Œ ë§Œë“œëŠ”ê°€? 

- Simple Gen
- RAGAS Gen 


# ì¢‹ì€ RAG í‰ê°€ ë°ì´í„°ë¥¼ ë§Œë“œëŠ” ë°©ë²• 


- ë°ì´í„° ìƒì„±ì„ ì „ë¶€ ìë™í™”í•˜ëŠ” ê²ƒì€ ê±°ì˜ ë¶ˆê°€ëŠ¥ 
- ê°€ì¥ ì¢‹ì€ ë°©ë²•ì€ "ì‹¤ì œ ìœ ì € ë°ì´í„° í™œìš©"
- ë‘ë²ˆì§¸ë¡œëŠ” "ë„ë©”ì¸ ì „ë¬¸ê°€ì™€ ë§Œë“œëŠ” ê²ƒ"
- ì„¸ë²ˆì§¸ ì˜µì…˜ì´ "Human In the Loop" ì „ëµ 
- í‰ê°€ ë°ì´í„° ì…‹ì„ ë§Œë“œëŠ” ì¼ì€ ê³ í†µ ìŠ¤ëŸ½ì§€ë§Œ, ë§ì€ ì‹œê°„ê³¼ ë…¸ë ¥ì„ íˆ¬ì…í•´ì•¼í•¨ 

# Human in the loop QA Gen

- LLMìœ¼ë¡œ QA ìƒì„± 
- ì‚¬ëŒì˜ ê²€ì¦ 
- í”„ë¡¬í”„íŠ¸ ì¬ì‘ì„± 

# LLM as Judge 

- í‰ê°€ì˜ ëª©ì 
  - Prompt for Evaluation 
    - ì–¸ì–´ëª¨ë¸ì— ì˜í•´ì„œ í‰ê°€í•˜ì 

- The default prompt for pairwise comparison

``` 
System
Please act as an impartial judge and evaluate the quality of the responses provided by two AI assistants to the user question below. 
Please read the userâ€™s question and the provided responses. 
Then follow the instructions and answer the userâ€™s question better. 
Provide explanations for your reasons such as the helpfulness, 
relevance, accuracy, depth, creativity, and level of detail of their responses. 
Begin your evaluation by comparing the two responses and provide a short explanation. 
Avoid any position biases and ensure that the order in which the responses were presented does not influence your decision. 
Do not allow the length of the responses to influence your evaluation. 
Do not favor certain names of the assistants. Be as objective as possible. After providing your explanation, 
output your final verdict by strictly using the format: â€œ[answerA]â€ if assistant A is better, â€œ[answerB]â€ if assistant B is better, and â€œ[C]â€ for a tie.

User Question
[question]

The Start of Assistant Aâ€™s Answer
A
[The End of Assistant Aâ€™s Answer]

The Start of Assistant Bâ€™s Answer
B
[The End of Assistant Bâ€™s Answer]

Figure 5: The default prompt for evaluation
```

``` 
[System]
You are a helpful assistant in evaluating the quality of the outputs for a given instruction. Your goal is to select the best output for the given instruction.

[User Input]
Select the Output (a) or Output (b) that is better for the given instruction. 
You should answer only using "Output (a)" or "Output (b). DO NOT output any other words.

Here are some rules of the evaluation:
(1) You should interpret whether the output honestly/precisely/closely follows the instruction, consider its helpfulness, accuracy, level of detail, harmlessness, etc.
(2) Outputs should not contain more/less than what the instruction asks for, as such outputs do NOT precisely execute the instruction.
(3) You should avoid any potential bias and your judgment should be as objective as possible. Do not presume that the order in which the outputs were presented should not affect your judgment, as Output (a) and Output (b) are **equally likely** to be the better one.

DO NOT provide any explanation for your choice.
DO NOT say both are nearly good.
You should answer using only "Output (a)" or "Output (b). DO NOT output any other words.

# Instruction:
{context}

# Output (a):
{output1}

# Output (b):
{output2}

# Which is better, Output (a) or Output (b)? Your response should be either "Output (a)" or "Output (b)".
```

> https://lmarena.ai/


# í‰ê°€ì— ëŒ€í•œ ê°€ì´ë“œë¼ì¸ì´ ë„ˆë¬´ ëª¨í˜¸í•´ì„œëŠ” ì•ˆëœë‹¤.

``` 
Consider the input prompt in the <input> tags and the output answer in the <output> tags, the context in the <prompt_criteria> tags, and the answer evaluation criteria in the <answer_criteria> tags.

<prompt_criteria>
The prompt should be clear, direct, and detailed.
The question, task, or goal should be well explained and be grammatically correct.
The prompt is better if containing examples.
The prompt is better if it specifies a role or sets a context.
The prompt is better if it provides details about the format and tone of the expected answer.
</prompt_criteria>
```

``` 
answer-score : ë‹µë³€ í‰ê°€ ì ìˆ˜ ( 0 ~ 100 ) - í™˜ê°ì´ ìˆìœ¼ë©´ ì ìˆ˜ê°€ í¬ê²Œ ê°ì†Œ 
prompt-score : í”„ë¡¬í”„íŠ¸ í‰ê°€ ì ìˆ˜ ( 0 ~ 100 ) 
justification : í‰ê°€ì— ëŒ€í•œ ì´ìœ  
input ë° output : í‰ê°€ ëŒ€ìƒ ì½˜í…ì¸ ë¥¼ ê·¸ëŒ€ë¡œ ë°˜í™˜ 
prompt-recommendations : í”„ë¡¬í”„íŠ¸ ê°œì„ ì„ ìœ„í•œ êµ¬ì²´ì ì¸ ê¶Œì¥ ì‚¬í•­ 
```

# í‰ê°€ í™˜ê²½ ì„¤ì • 

- Function Call í•¨ìˆ˜ ì •ì˜

# ëª¨ë¸ë³„ë¡œ ì¢‹ì•„í•˜ëŠ” ì ìˆ˜ ë²”ìœ„ê°€ ë”°ë¡œ ìˆì„ ìˆ˜ ìˆìŒ 

- LLMì€ ì—°ì†ì ì¸ ë²”ìœ„ í‰ê°€ì— ì•½í•˜ê¸° ë•Œë¬¸ì— LLM ë¶„ë¥˜ í‰ê°€ì— ë°©ì‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ ì¢‹ìŒ

# ëª¨ë¸ì´ ì‘ì„ ìˆ˜ë¡ í¸í–¥ì´ ë” ë§ì´ ìƒê¸´ë‹¤. 

Promptì˜ ë‚´ìš©ì— ë”°ë¼ì„œ ê²°ê³¼ê°€ ë§ì´ ë‹¬ë¼ì§ˆìˆ˜ ìˆë‹¤. 

# SoftMax Function 

- https://syj9700.tistory.com/38

# ğŸ“ AI-based Paraphrasing Techniques

Paraphrasing is the task of expressing the same meaning using different words or sentence structures. It is a key component in many natural language processing (NLP) applications such as summarization, chatbot response generation, question generation, and more.

---

## ğŸš€ Methods of Paraphrasing in AI

### 1. Rule-based Paraphrasing
- Utilizes grammar rules, synonym replacement, and voice transformation (active â‡„ passive).
- **Example**:  
  `He wrote a book.` â†’ `A book was written by him.`
- âœ… Simple and controllable
- âŒ Limited diversity, prone to semantic distortion

---

### 2. Machine Learning-based Paraphrasing
- Learns sentence structures and patterns from large parallel datasets.

#### Techniques:
- **Seq2Seq (RNN/LSTM)**: Encodes and decodes sequences to generate paraphrases.
- **Back-Translation**: Translates the original sentence into another language, then back to the original language.
    - Example: English â†’ French â†’ English

---

### 3. Pretrained Language Model-based Paraphrasing âœ… *Most commonly used today*
- Leverages large models like **BERT**, **GPT**, **T5**, **Pegasus**, etc.

#### Examples:
- **T5 (Text-to-Text Transfer Transformer)**  
  Input: `paraphrase: The weather is great today.`  
  Output: `It's a beautiful day.`

- **GPT-4 / ChatGPT**  
  Prompt-based generation:
  > `"Paraphrase this sentence: 'She completed the project on time.'"`  
  â†’ `"The project was finished by her within the deadline."`

---

### 4. Embedding-based Semantic Paraphrasing
- Converts sentences into embedding vectors and retrieves semantically similar sentences from a vector database.

#### Applications:
- Semantic search (e.g., FAQ matching)
- Retrieving diverse question formulations

# Tips 

- ëŒ€ë¬¸ì ì“°ì§€ ë§ ê²ƒ 
- ë¶€ì •ì–´ ì“°ì§€ ë§ ê²ƒ 
- ì„œë¹„ìŠ¤ë¥¼ ìš´ì˜í•˜ë©´ì„œ í–‰ë™ ì§€í‘œë¥¼ ë³´ëŠ” ê²ƒì´ ë§ì´ ë„ì›€ì´ ë  ìˆ˜ ìˆìŒ 
- SLMì˜ ê²½ìš°ì—ë„ ë‹¤ì–‘í•œ Referencesë¥¼ SLM Prompt ê¸°ì¤€ìœ¼ë¡œ ì°¸ì¡° í•´ì•¼í•¨. 
- ì‘ì€ ëª¨ë¸ì¼ ìˆ˜ë¡ Reasoning Stepì— ë” ë¯¼ê°í•˜ë‹¤. 

# References 

> https://smith.langchain.com/hub/homanp/human-in-the-loop?organizationId=6722a1b7-48f3-4ec9-a5ac-e04155487f6a&tab=0    
> https://docs.smith.langchain.com/prompt_engineering/how_to_guides/manage_prompts_programatically   
> https://wikidocs.net/262593  
> https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/  
> https://langchain-ai.github.io/langgraph/concepts/human_in_the_loop/#how-does-resuming-from-an-interrupt-work  
> https://github.com/UpstageAI/solar-prompt-cookbook